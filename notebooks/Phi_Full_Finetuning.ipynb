{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b48e46e-e90e-4d8b-b42a-3b12842464d0",
      "metadata": {
        "id": "2b48e46e-e90e-4d8b-b42a-3b12842464d0"
      },
      "outputs": [],
      "source": [
        "# Download the Question-answer pair dataset\n",
        "\n",
        "!gdown https://drive.google.com/uc?id=1t30Elo92Ti8F3BOorJtVGF4MQfjMskk-\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7f472dc-3733-47e0-8f89-7e627b4e4f1b",
      "metadata": {
        "id": "c7f472dc-3733-47e0-8f89-7e627b4e4f1b"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "\n",
        "%pip install transformers accelerate peft bitsandbytes datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e69585f1-6788-4ead-93db-996fa1847d74",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "661bc0611a93417f86af4168565fabc5"
          ]
        },
        "id": "e69585f1-6788-4ead-93db-996fa1847d74",
        "outputId": "319e5a16-524b-471d-f8c0-bab7d5679d1f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-10-31 17:29:07.399204: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-10-31 17:29:07.411802: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-31 17:29:07.426826: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-31 17:29:07.431374: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-31 17:29:07.442747: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-31 17:29:12.997300: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "661bc0611a93417f86af4168565fabc5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Set Hugging Face token and import necessary libraries\n",
        "\n",
        "HF_TOKEN = ''\n",
        "import torch\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    GenerationConfig,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "\n",
        "# Load model on GPU\n",
        "device_map = {\"\": 0}\n",
        "\n",
        "# Load tokenizer from the pre-trained 'microsoft/Phi-3-mini-4k-instruct'\n",
        "tokenizer = AutoTokenizer.from_pretrained('microsoft/Phi-3-mini-4k-instruct',trust_remote_code=True,padding_side=\"left\",add_eos_token=True,add_bos_token=True,use_fast=False,\n",
        "    token=HF_TOKEN)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    'microsoft/Phi-3-mini-4k-instruct',\n",
        "    device_map=device_map,\n",
        "    trust_remote_code=True,\n",
        "    token=HF_TOKEN)\n",
        "\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bbd8f2c-7c97-44c4-8d19-b1608d07c6fe",
      "metadata": {
        "id": "0bbd8f2c-7c97-44c4-8d19-b1608d07c6fe"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import pyarrow as pa\n",
        "\n",
        "# Load the dataset\n",
        "data_path = \"cwe_prompt_completion.json\"\n",
        "dataset = load_dataset('json', data_files=data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17fbdebe-a755-4773-9f82-01aef7570719",
      "metadata": {
        "id": "17fbdebe-a755-4773-9f82-01aef7570719"
      },
      "outputs": [],
      "source": [
        "def format_sample_text(sample):\n",
        "    # Introductory statement and markers\n",
        "    introduction = \"The following is a question about a security vulnerability. Please give a thorough and accurate answer.\"\n",
        "    prompt_section = f\"### Prompt:\\n{sample.get('prompt', '')}\".strip()  # Strip to remove any extra spaces\n",
        "    response_section = f\"### Response:\\n{sample.get('completion', '')}\".strip()\n",
        "    conclusion = \"### End\"\n",
        "\n",
        "    # Collect all non-empty parts\n",
        "    sections = [introduction, prompt_section, response_section, conclusion]\n",
        "    formatted_text = \"\\n\\n\".join(filter(lambda x: x, sections))  # Only include non-empty sections\n",
        "\n",
        "    sample[\"text\"] = formatted_text\n",
        "    return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b386c257-9519-411e-97a7-6c94d94436c3",
      "metadata": {
        "id": "b386c257-9519-411e-97a7-6c94d94436c3"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
        "def get_max_length(model):\n",
        "    conf = model.config\n",
        "    max_length = None\n",
        "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
        "        max_length = getattr(model.config, length_setting, None)\n",
        "        if max_length:\n",
        "            print(f\"Found max lenth: {max_length}\")\n",
        "            break\n",
        "    if not max_length:\n",
        "        max_length = 1024\n",
        "        print(f\"Using default max length: {max_length}\")\n",
        "    return max_length\n",
        "\n",
        "\n",
        "def preprocess_batch(batch, tokenizer, max_length):\n",
        "    \"\"\"\n",
        "    Tokenizing a batch\n",
        "    \"\"\"\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
        "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int,seed, dataset):\n",
        "    \"\"\"Format & tokenize it so it is ready for training\n",
        "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
        "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
        "    \"\"\"\n",
        "\n",
        "    # Add prompt to each sample\n",
        "    print(\"Preprocessing dataset...\")\n",
        "    dataset = dataset.map(format_sample_text)\n",
        "\n",
        "    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields\n",
        "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
        "    dataset = dataset.map(\n",
        "        _preprocessing_function,\n",
        "        batched=True\n",
        "    )\n",
        "\n",
        "    # Filter out samples that have input_ids exceeding max_length\n",
        "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
        "\n",
        "    # Shuffle dataset\n",
        "    dataset = dataset.shuffle(seed=seed)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5169bb79-d74d-447b-9a6b-2a8f38bdfca3",
      "metadata": {
        "id": "5169bb79-d74d-447b-9a6b-2a8f38bdfca3"
      },
      "outputs": [],
      "source": [
        "## Pre-process dataset\n",
        "from transformers import set_seed\n",
        "seed = 42\n",
        "set_seed(seed)\n",
        "max_length = get_max_length(model)\n",
        "print(max_length)\n",
        "train_test_split = dataset['train'].train_test_split(test_size=0.2)\n",
        "\n",
        "#TODO reverted to orignal code - removal of columns, there is no validation set in the dataset, you can potentially fix it or ignore it completely\n",
        "train_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['train'])\n",
        "eval_dataset = preprocess_dataset(tokenizer, max_length,seed, train_test_split['test'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4717de55-3399-4718-b467-0c17b57266b6",
      "metadata": {
        "id": "4717de55-3399-4718-b467-0c17b57266b6"
      },
      "outputs": [],
      "source": [
        "from peft import get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "#Prepare the model for efficient training with k-bit precision\n",
        "model = prepare_model_for_kbit_training(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b6f6241-1ca9-477a-9217-275d9a130f23",
      "metadata": {
        "id": "5b6f6241-1ca9-477a-9217-275d9a130f23"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "\n",
        "# Tokenize the dataset\n",
        "tokenized_datasets = train_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Prepare the dataset for PyTorch\n",
        "tokenized_datasets.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d56da2cf-069d-4cbc-973e-268436c91c62",
      "metadata": {
        "id": "d56da2cf-069d-4cbc-973e-268436c91c62"
      },
      "outputs": [],
      "source": [
        "for param in model.parameters():\n",
        "    param.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbdfa85d-b5ad-4ba0-ae75-bf0671a3bff5",
      "metadata": {
        "id": "dbdfa85d-b5ad-4ba0-ae75-bf0671a3bff5",
        "outputId": "e0ca4a66-7022-441a-bdb8-54832b2a4dfa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "/home3/s5054702/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "You are not running the flash-attention implementation, expect numerical differences.\n",
            "/home3/s5054702/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2000/2000 2:50:52, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.106800</td>\n",
              "      <td>1.109053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.952700</td>\n",
              "      <td>0.959703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.904000</td>\n",
              "      <td>0.895429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.862700</td>\n",
              "      <td>0.854669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.847700</td>\n",
              "      <td>0.820740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.825100</td>\n",
              "      <td>0.796678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.819300</td>\n",
              "      <td>0.777079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.791300</td>\n",
              "      <td>0.762171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.813300</td>\n",
              "      <td>0.752755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.812200</td>\n",
              "      <td>0.748963</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home3/s5054702/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/home3/s5054702/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/home3/s5054702/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/home3/s5054702/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/home3/s5054702/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/home3/s5054702/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/home3/s5054702/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/home3/s5054702/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/home3/s5054702/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/home3/s5054702/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/home3/s5054702/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/home3/s5054702/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/home3/s5054702/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/home3/s5054702/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/home3/s5054702/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/home3/s5054702/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/home3/s5054702/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/home3/s5054702/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('./Phi-3-mini-4k-instruct/tokenizer_config.json',\n",
              " './Phi-3-mini-4k-instruct/special_tokens_map.json',\n",
              " './Phi-3-mini-4k-instruct/tokenizer.model',\n",
              " './Phi-3-mini-4k-instruct/added_tokens.json')"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "peft_training_args = TrainingArguments(\n",
        "    output_dir='./Phi-3-mini-4k-instruct',\n",
        "    warmup_steps=200,  # Increased to 10% of total steps\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,  # Adjusted to keep effective batch size manageable\n",
        "    max_steps=2000,\n",
        "    learning_rate=5e-6,  # Experimented with a slightly lower learning rate\n",
        "    optim=\"adamw_hf\",  # Tried regular AdamW for more stable training\n",
        "    logging_steps=50,  # Logged more frequently to monitor progress\n",
        "    logging_dir=\"./logs\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=200,  # Saved more frequently to monitor performance\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=200,  # Evaluated more frequently\n",
        "    do_eval=True,\n",
        "    gradient_checkpointing=True,\n",
        "    bf16=True,  # good for A100 hardware\n",
        "    report_to=\"none\",\n",
        "    overwrite_output_dir=True,\n",
        "    group_by_length=True,\n",
        "    save_total_limit=3  # Limit number of checkpoints to save space\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=peft_training_args,\n",
        "    eval_dataset=eval_dataset,\n",
        "    train_dataset=train_dataset,\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "\n",
        "# Start fine-tuning the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"./Phi-3-mini-4k-instruct\")\n",
        "tokenizer.save_pretrained(\"./Phi-3-mini-4k-instruct\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5654fd0-3c47-4a11-aeff-96461be8614b",
      "metadata": {
        "id": "c5654fd0-3c47-4a11-aeff-96461be8614b",
        "outputId": "1d73e74e-a978-422e-8e16-2626c8ee32f0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Zipping: 100%|██████████| 15.3G/15.3G [10:40<00:00, 23.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "\n",
        "def zip_with_progress(folder_path, output_path):\n",
        "    # Calculate the total size of the folder to zip\n",
        "    total_size = 0\n",
        "    for dirpath, dirnames, filenames in os.walk(folder_path):\n",
        "        for f in filenames:\n",
        "            fp = os.path.join(dirpath, f)\n",
        "            total_size += os.path.getsize(fp)\n",
        "\n",
        "    # Zip the folder and show progress using tqdm\n",
        "    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        with tqdm(total=total_size, unit='B', unit_scale=True, desc=\"Zipping\") as pbar:\n",
        "            for dirpath, dirnames, filenames in os.walk(folder_path):\n",
        "                for f in filenames:\n",
        "                    fp = os.path.join(dirpath, f)\n",
        "                    # Add file to the zip archive\n",
        "                    zipf.write(fp, os.path.relpath(fp, folder_path))\n",
        "                    # Update progress bar based on file size\n",
        "                    pbar.update(os.path.getsize(fp))\n",
        "\n",
        "# Example usage:\n",
        "folder_path = 'Phi-3-mini-4k-instruct'  # Replace with your folder path\n",
        "output_path = 'Phi-3-finetune5.zip'  # Replace with the zip file path\n",
        "zip_with_progress(folder_path, output_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5549a90d-ede9-427f-9a0c-603ef840092a",
      "metadata": {
        "id": "5549a90d-ede9-427f-9a0c-603ef840092a",
        "outputId": "06b2f6c1-5790-4c6f-8cbd-a7becc45c4d6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<a href='Phi-3-mini-finetune.zip' target='_blank'>Phi-3-mini-finetune.zip</a><br>"
            ],
            "text/plain": [
              "/scratch/s5054702/Phi-3-mini-finetune.zip"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from IPython.display import FileLink\n",
        "\n",
        "# Path to the zipped file\n",
        "\n",
        "# Display a download link\n",
        "FileLink(output_path)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm2",
      "language": "python",
      "name": "llm2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
