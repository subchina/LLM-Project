{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b48e46e-e90e-4d8b-b42a-3b12842464d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1t30Elo92Ti8F3BOorJtVGF4MQfjMskk-\n",
      "To: /scratch/s5054702/cwe_prompt_completion.json\n",
      "100%|██████████████████████████████████████| 16.6M/16.6M [00:00<00:00, 56.5MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Download Dataset\n",
    "!gdown https://drive.google.com/uc?id=1t30Elo92Ti8F3BOorJtVGF4MQfjMskk-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f472dc-3733-47e0-8f89-7e627b4e4f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home3/s5054702/.local/lib/python3.10/site-packages (4.45.2)\n",
      "Requirement already satisfied: accelerate in /home3/s5054702/.local/lib/python3.10/site-packages (0.34.2)\n",
      "Requirement already satisfied: peft in /home3/s5054702/.local/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: bitsandbytes in /home3/s5054702/.local/lib/python3.10/site-packages (0.44.1)\n",
      "Requirement already satisfied: datasets in /home3/s5054702/.local/lib/python3.10/site-packages (3.0.1)\n",
      "Requirement already satisfied: filelock in /home3/s5054702/.local/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home3/s5054702/.local/lib/python3.10/site-packages (from transformers) (0.24.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /home3/s5054702/.local/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home3/s5054702/.local/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home3/s5054702/.local/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/intel/icelake/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (from transformers) (2022.4.24)\n",
      "Requirement already satisfied: requests in /home3/s5054702/.local/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home3/s5054702/.local/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home3/s5054702/.local/lib/python3.10/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home3/s5054702/.local/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: psutil in /cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/intel/icelake/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home3/s5054702/.local/lib/python3.10/site-packages (from accelerate) (2.4.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home3/s5054702/.local/lib/python3.10/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home3/s5054702/.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home3/s5054702/.local/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /home3/s5054702/.local/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home3/s5054702/.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /home3/s5054702/.local/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /home3/s5054702/.local/lib/python3.10/site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home3/s5054702/.local/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home3/s5054702/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home3/s5054702/.local/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home3/s5054702/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home3/s5054702/.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home3/s5054702/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.15.5)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home3/s5054702/.local/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home3/s5054702/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home3/s5054702/.local/lib/python3.10/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home3/s5054702/.local/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/intel/icelake/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home3/s5054702/.local/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: sympy in /home3/s5054702/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: networkx in /home3/s5054702/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /home3/s5054702/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home3/s5054702/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home3/s5054702/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home3/s5054702/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home3/s5054702/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home3/s5054702/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home3/s5054702/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home3/s5054702/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home3/s5054702/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home3/s5054702/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home3/s5054702/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home3/s5054702/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home3/s5054702/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home3/s5054702/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home3/s5054702/.local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home3/s5054702/.local/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home3/s5054702/.local/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home3/s5054702/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home3/s5054702/.local/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home3/s5054702/.local/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home3/s5054702/.local/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#Install Requirements\n",
    "%pip install transformers accelerate peft bitsandbytes datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f688ba0-53d8-4eed-bd9d-ca360b091259",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert HF TOKEN\n",
    "HF_TOKEN = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e69585f1-6788-4ead-93db-996fa1847d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-22 15:26:19.718550: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-22 15:26:20.919543: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-22 15:26:21.392536: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-22 15:26:21.539588: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-22 15:26:22.311560: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-22 15:26:33.511144: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e41ee50ac2141bb850df4365f128b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    GenerationConfig,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "\n",
    "# Set data type for computations to be float16 \n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "# Bitsandbytes 4bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  \n",
    "    bnb_4bit_quant_type='nf4',  # NormalFloat4 (NF4)\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=False  # disables double quantization (-precision +efficiency)\n",
    ")\n",
    "\n",
    "# Load model on GPU\n",
    "device_map = {\"\": 0} \n",
    "\n",
    "# Load tokenizer from the pre-trained 'microsoft/Phi-3-mini-4k-instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'microsoft/Phi-3-mini-4k-instruct',\n",
    "    trust_remote_code=True,  \n",
    "    padding_side=\"left\",  \n",
    "    add_eos_token=True,  \n",
    "    add_bos_token=True, \n",
    "    use_fast=False,  \n",
    "    token=HF_TOKEN \n",
    ")\n",
    "\n",
    "# Load the pre-trained model with quantization settings\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'microsoft/Phi-3-mini-4k-instruct',\n",
    "    device_map=device_map,  # Specify device\n",
    "    quantization_config=bnb_config,  # Apply bnb defined 4-bit quantization\n",
    "    trust_remote_code=True,  \n",
    "    token=HF_TOKEN  \n",
    ")\n",
    "\n",
    "# Set up the LoRA (Low-Rank Adaptation) config\n",
    "lora_config = LoraConfig(\n",
    "    r=64,  # Rank of LoRA matrix controls size of LoRA matrices.the bigger the size the more precise but energy requiring\n",
    "    lora_alpha=16,  # control the impact of LoRA stregnth \n",
    "    target_modules= ['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"],  # Apply LoRA to specific layers in Phi\n",
    "    lora_dropout=0.05,  # Dropout rate for regularization\n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Apply the LoRA configuration to the quantized model for efficient fine-tuning\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Enable gradient checkpointing to save memory during training\n",
    "model.gradient_checkpointing_enable()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bbd8f2c-7c97-44c4-8d19-b1608d07c6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pyarrow as pa\n",
    "\n",
    "# Load the dataset\n",
    "data_path = \"cwe_prompt_completion.json\"\n",
    "dataset = load_dataset('json', data_files=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17fbdebe-a755-4773-9f82-01aef7570719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_sample_text(sample):\n",
    "    # Introductory statement and markers\n",
    "    introduction = \"The following is a question about a security vulnerability. Please give a thorough and accurate answer.\"\n",
    "    prompt_section = f\"### Prompt:\\n{sample.get('prompt', '')}\".strip()  # Strip to remove any extra spaces\n",
    "    response_section = f\"### Response:\\n{sample.get('completion', '')}\".strip()\n",
    "    conclusion = \"### End\"\n",
    "\n",
    "    # Collect all non-empty parts\n",
    "    sections = [introduction, prompt_section, response_section, conclusion]\n",
    "    formatted_text = \"\\n\\n\".join(filter(lambda x: x, sections))  # Only include non-empty sections\n",
    "\n",
    "    sample[\"text\"] = formatted_text\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b386c257-9519-411e-97a7-6c94d94436c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def get_max_length(model):\n",
    "    conf = model.config\n",
    "    max_length = None\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length\n",
    "\n",
    "\n",
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizing a batch\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int,seed, dataset):\n",
    "    \"\"\"Format & tokenize it so it is ready for training\n",
    "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(format_sample_text)\n",
    "\n",
    "    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have input_ids exceeding max_length\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "\n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5169bb79-d74d-447b-9a6b-2a8f38bdfca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found max lenth: 32768\n",
      "32768\n",
      "Preprocessing dataset...\n",
      "Preprocessing dataset...\n"
     ]
    }
   ],
   "source": [
    "## Pre-process dataset\n",
    "from transformers import set_seed\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "max_length = get_max_length(model)\n",
    "print(max_length)\n",
    "train_test_split = dataset['train'].train_test_split(test_size=0.2)\n",
    "\n",
    "#TODO reverted to orignal code - removal of columns, there is no validation set in the dataset, you can potentially fix it or ignore it completely\n",
    "train_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['train'])\n",
    "eval_dataset = preprocess_dataset(tokenizer, max_length,seed, train_test_split['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4717de55-3399-4718-b467-0c17b57266b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home3/s5054702/.local/lib/python3.10/site-packages/peft/mapping.py:172: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'mistralai/Mistral-7B-v0.1' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "#Prepare the model for efficient training with k-bit precision\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "#Apply LoRA configuration to model\n",
    "model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdfa85d-b5ad-4ba0-ae75-bf0671a3bff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir = './phi-finetune',\n",
    "    warmup_steps=100,  # Increased warmup steps to stabilize training(10%)\n",
    "    per_device_train_batch_size=32,  # Increaseed batch size, adjusted based on A100 memory\n",
    "    gradient_accumulation_steps=2,  # Adjust gradient accumulation for larger batches\n",
    "    max_steps=1000,\n",
    "    learning_rate=3e-4,\n",
    "    optim=\"paged_adamw_8bit\", \n",
    "    logging_steps=25,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,  # Saving less frequently to reduce I/O overhead\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,  # Evaluate less frequently to focus on training\n",
    "    do_eval=True,\n",
    "    gradient_checkpointing=True,  # Keep this for saving memory during training\n",
    "    bf16=True,  #set this to True (for A100 hardware)\n",
    "    report_to=\"none\",\n",
    "    overwrite_output_dir=True,\n",
    "    group_by_length=True,\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=peft_training_args,\n",
    "    eval_dataset=eval_dataset,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    ")\n",
    "\n",
    "# Start fine-tuning the model\n",
    "trainer.train()\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./phi-qlora-finetune\")\n",
    "tokenizer.save_pretrained(\"./phi-qlora-finetune\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5654fd0-3c47-4a11-aeff-96461be8614b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Zipping: 100%|██████████| 152M/152M [00:07<00:00, 21.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "def zip_with_progress(folder_path, output_path):\n",
    "    # Calculate the total size of the folder to zip\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total_size += os.path.getsize(fp)\n",
    "\n",
    "    # Zip the folder and show progress using tqdm\n",
    "    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        with tqdm(total=total_size, unit='B', unit_scale=True, desc=\"Zipping\") as pbar:\n",
    "            for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "                for f in filenames:\n",
    "                    fp = os.path.join(dirpath, f)\n",
    "                    # Add file to the zip archive\n",
    "                    zipf.write(fp, os.path.relpath(fp, folder_path))\n",
    "                    # Update progress bar based on file size\n",
    "                    pbar.update(os.path.getsize(fp))\n",
    "\n",
    "# Example usage:\n",
    "folder_path = 'mistral-qlora-finetune'  # Replace with your folder path\n",
    "output_path = 'mistral-qlora-finetune.zip'  # Replace with the zip file path\n",
    "zip_with_progress(folder_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5549a90d-ede9-427f-9a0c-603ef840092a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "# Path to the zipped file\n",
    "output_path = 'mistral-finetune.zip'\n",
    "\n",
    "# Display a download link\n",
    "FileLink(output_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm2",
   "language": "python",
   "name": "llm2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
