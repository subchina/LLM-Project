{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b48e46e-e90e-4d8b-b42a-3b12842464d0",
      "metadata": {
        "id": "2b48e46e-e90e-4d8b-b42a-3b12842464d0"
      },
      "outputs": [],
      "source": [
        "# Download Dataset\n",
        "!gdown https://drive.google.com/uc?id=1t30Elo92Ti8F3BOorJtVGF4MQfjMskk-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7f472dc-3733-47e0-8f89-7e627b4e4f1b",
      "metadata": {
        "id": "c7f472dc-3733-47e0-8f89-7e627b4e4f1b"
      },
      "outputs": [],
      "source": [
        "#Install Requirements\n",
        "%pip install transformers accelerate peft bitsandbytes datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f688ba0-53d8-4eed-bd9d-ca360b091259",
      "metadata": {
        "id": "2f688ba0-53d8-4eed-bd9d-ca360b091259"
      },
      "outputs": [],
      "source": [
        "#Insert HF TOKEN\n",
        "HF_TOKEN = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e69585f1-6788-4ead-93db-996fa1847d74",
      "metadata": {
        "id": "e69585f1-6788-4ead-93db-996fa1847d74"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    GenerationConfig,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "\n",
        "# Set data type for computations to be float16\n",
        "compute_dtype = getattr(torch, \"float16\")\n",
        "\n",
        "# Bitsandbytes 4bit quantization config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',  # NormalFloat4 (NF4)\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=False  # disables double quantization (-precision +efficiency)\n",
        ")\n",
        "\n",
        "# Load model on GPU\n",
        "device_map = {\"\": 0}\n",
        "\n",
        "# Load tokenizer from the pre-trained 'mistralai/Mistral-7B-v0.1'\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    'mistralai/Mistral-7B-v0.1',\n",
        "    trust_remote_code=True,\n",
        "    padding_side=\"left\",\n",
        "    add_eos_token=True,\n",
        "    add_bos_token=True,\n",
        "    use_fast=False,\n",
        "    token=HF_TOKEN\n",
        ")\n",
        "\n",
        "# Load the pre-trained model with quantization settings\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    'mistralai/Mistral-7B-v0.1',\n",
        "    device_map=device_map,  # Specify device\n",
        "    quantization_config=bnb_config,  # Apply bnb defined 4-bit quantization\n",
        "    trust_remote_code=True,\n",
        "    token=HF_TOKEN\n",
        ")\n",
        "\n",
        "# Set up the LoRA (Low-Rank Adaptation) config\n",
        "lora_config = LoraConfig(\n",
        "    r=64,  # Rank of LoRA matrix controls size of LoRA matrices.the bigger the size the more precise but energy requiring\n",
        "    lora_alpha=16,  # control the impact of LoRA stregnth\n",
        "    target_modules=['q_proj', 'v_proj', 'k_proj', 'dense'],  # Apply LoRA to specific layers in Mistral\n",
        "    lora_dropout=0.05,  # Dropout rate for regularization\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Apply the LoRA configuration to the quantized model for efficient fine-tuning\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Enable gradient checkpointing to save memory during training\n",
        "model.gradient_checkpointing_enable()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bbd8f2c-7c97-44c4-8d19-b1608d07c6fe",
      "metadata": {
        "id": "0bbd8f2c-7c97-44c4-8d19-b1608d07c6fe"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import pyarrow as pa\n",
        "\n",
        "# Load the dataset\n",
        "data_path = \"cwe_prompt_completion.json\"\n",
        "dataset = load_dataset('json', data_files=data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17fbdebe-a755-4773-9f82-01aef7570719",
      "metadata": {
        "id": "17fbdebe-a755-4773-9f82-01aef7570719"
      },
      "outputs": [],
      "source": [
        "def format_sample_text(sample):\n",
        "    # Introductory statement and markers\n",
        "    introduction = \"The following is a question about a security vulnerability. Please give a thorough and accurate answer.\"\n",
        "    prompt_section = f\"### Prompt:\\n{sample.get('prompt', '')}\".strip()  # Strip to remove any extra spaces\n",
        "    response_section = f\"### Response:\\n{sample.get('completion', '')}\".strip()\n",
        "    conclusion = \"### End\"\n",
        "\n",
        "    # Collect all non-empty parts\n",
        "    sections = [introduction, prompt_section, response_section, conclusion]\n",
        "    formatted_text = \"\\n\\n\".join(filter(lambda x: x, sections))  # Only include non-empty sections\n",
        "\n",
        "    sample[\"text\"] = formatted_text\n",
        "    return sample\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b386c257-9519-411e-97a7-6c94d94436c3",
      "metadata": {
        "id": "b386c257-9519-411e-97a7-6c94d94436c3"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
        "def get_max_length(model):\n",
        "    conf = model.config\n",
        "    max_length = None\n",
        "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
        "        max_length = getattr(model.config, length_setting, None)\n",
        "        if max_length:\n",
        "            print(f\"Found max lenth: {max_length}\")\n",
        "            break\n",
        "    if not max_length:\n",
        "        max_length = 1024\n",
        "        print(f\"Using default max length: {max_length}\")\n",
        "    return max_length\n",
        "\n",
        "\n",
        "def preprocess_batch(batch, tokenizer, max_length):\n",
        "    \"\"\"\n",
        "    Tokenizing a batch\n",
        "    \"\"\"\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
        "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int,seed, dataset):\n",
        "    \"\"\"Format & tokenize it so it is ready for training\n",
        "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
        "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
        "    \"\"\"\n",
        "\n",
        "    # Add prompt to each sample\n",
        "    print(\"Preprocessing dataset...\")\n",
        "    dataset = dataset.map(format_sample_text)\n",
        "\n",
        "    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields\n",
        "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
        "    dataset = dataset.map(\n",
        "        _preprocessing_function,\n",
        "        batched=True\n",
        "    )\n",
        "\n",
        "    # Filter out samples that have input_ids exceeding max_length\n",
        "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
        "\n",
        "    # Shuffle dataset\n",
        "    dataset = dataset.shuffle(seed=seed)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5169bb79-d74d-447b-9a6b-2a8f38bdfca3",
      "metadata": {
        "id": "5169bb79-d74d-447b-9a6b-2a8f38bdfca3",
        "outputId": "5fde4fc1-808c-4640-802f-dd7d18876ac1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found max lenth: 32768\n",
            "32768\n",
            "Preprocessing dataset...\n",
            "Preprocessing dataset...\n"
          ]
        }
      ],
      "source": [
        "## Pre-process dataset\n",
        "from transformers import set_seed\n",
        "seed = 42\n",
        "set_seed(seed)\n",
        "max_length = get_max_length(model)\n",
        "print(max_length)\n",
        "train_test_split = dataset['train'].train_test_split(test_size=0.2)\n",
        "\n",
        "#TODO reverted to orignal code - removal of columns, there is no validation set in the dataset, you can potentially fix it or ignore it completely\n",
        "train_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['train'])\n",
        "eval_dataset = preprocess_dataset(tokenizer, max_length,seed, train_test_split['test'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4717de55-3399-4718-b467-0c17b57266b6",
      "metadata": {
        "id": "4717de55-3399-4718-b467-0c17b57266b6",
        "outputId": "5e3e948e-1359-41ed-c6d8-3211728092b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home3/s5054702/.local/lib/python3.10/site-packages/peft/mapping.py:172: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'mistralai/Mistral-7B-v0.1' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from peft import get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "#Prepare the model for efficient training with k-bit precision\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "#Apply LoRA configuration to model\n",
        "model = get_peft_model(model, lora_config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbdfa85d-b5ad-4ba0-ae75-bf0671a3bff5",
      "metadata": {
        "id": "dbdfa85d-b5ad-4ba0-ae75-bf0671a3bff5",
        "outputId": "81c18ca6-c46d-4b9d-9cdb-93c4bf4683c3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home3/s5054702/.local/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/home3/s5054702/.local/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
            "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "/home3/s5054702/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1000/1000 5:29:04, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.303100</td>\n",
              "      <td>1.167140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.073200</td>\n",
              "      <td>1.050644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.977000</td>\n",
              "      <td>0.952905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.935200</td>\n",
              "      <td>0.923327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.905900</td>\n",
              "      <td>0.901676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.884300</td>\n",
              "      <td>0.884953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.888400</td>\n",
              "      <td>0.867254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.863100</td>\n",
              "      <td>0.854124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.863400</td>\n",
              "      <td>0.843524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.865700</td>\n",
              "      <td>0.832306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.802000</td>\n",
              "      <td>0.820870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.832300</td>\n",
              "      <td>0.811922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.812700</td>\n",
              "      <td>0.803658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.810500</td>\n",
              "      <td>0.795282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.809600</td>\n",
              "      <td>0.789533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.783300</td>\n",
              "      <td>0.782390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.792900</td>\n",
              "      <td>0.777100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.803000</td>\n",
              "      <td>0.772882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.790600</td>\n",
              "      <td>0.769638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.795800</td>\n",
              "      <td>0.768103</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home3/s5054702/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/home3/s5054702/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/home3/s5054702/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/home3/s5054702/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/home3/s5054702/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/home3/s5054702/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/home3/s5054702/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/home3/s5054702/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/home3/s5054702/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/home3/s5054702/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/home3/s5054702/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/home3/s5054702/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/home3/s5054702/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/home3/s5054702/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/home3/s5054702/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/home3/s5054702/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/home3/s5054702/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/home3/s5054702/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/home3/s5054702/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('./mistral-qlora-finetune/tokenizer_config.json',\n",
              " './mistral-qlora-finetune/special_tokens_map.json',\n",
              " './mistral-qlora-finetune/tokenizer.model',\n",
              " './mistral-qlora-finetune/added_tokens.json')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "peft_training_args = TrainingArguments(\n",
        "    output_dir = './mistral-finetune',\n",
        "    warmup_steps=100,  # Increased warmup steps to stabilize training(10%)\n",
        "    per_device_train_batch_size=32,  # Increaseed batch size, adjusted based on A100 memory\n",
        "    gradient_accumulation_steps=2,  # Adjust gradient accumulation for larger batches\n",
        "    max_steps=1000,\n",
        "    learning_rate=3e-4,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    logging_steps=25,\n",
        "    logging_dir=\"./logs\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50,  # Saving less frequently to reduce I/O overhead\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=50,  # Evaluate less frequently to focus on training\n",
        "    do_eval=True,\n",
        "    gradient_checkpointing=True,  # Keep this for saving memory during training\n",
        "    bf16=True,  #set this to True (for A100 hardware)\n",
        "    report_to=\"none\",\n",
        "    overwrite_output_dir=True,\n",
        "    group_by_length=True,\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=peft_training_args,\n",
        "    eval_dataset=eval_dataset,\n",
        "    train_dataset=train_dataset,\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "\n",
        ")\n",
        "\n",
        "# Start fine-tuning the model\n",
        "trainer.train()\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"./mistral-qlora-finetune\")\n",
        "tokenizer.save_pretrained(\"./mistral-qlora-finetune\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5654fd0-3c47-4a11-aeff-96461be8614b",
      "metadata": {
        "id": "c5654fd0-3c47-4a11-aeff-96461be8614b",
        "outputId": "2a98f536-fe40-4a03-c746-dfb4f426cf97"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Zipping: 100%|██████████| 152M/152M [00:07<00:00, 21.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "\n",
        "def zip_with_progress(folder_path, output_path):\n",
        "    # Calculate the total size of the folder to zip\n",
        "    total_size = 0\n",
        "    for dirpath, dirnames, filenames in os.walk(folder_path):\n",
        "        for f in filenames:\n",
        "            fp = os.path.join(dirpath, f)\n",
        "            total_size += os.path.getsize(fp)\n",
        "\n",
        "    # Zip the folder and show progress using tqdm\n",
        "    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        with tqdm(total=total_size, unit='B', unit_scale=True, desc=\"Zipping\") as pbar:\n",
        "            for dirpath, dirnames, filenames in os.walk(folder_path):\n",
        "                for f in filenames:\n",
        "                    fp = os.path.join(dirpath, f)\n",
        "                    # Add file to the zip archive\n",
        "                    zipf.write(fp, os.path.relpath(fp, folder_path))\n",
        "                    # Update progress bar based on file size\n",
        "                    pbar.update(os.path.getsize(fp))\n",
        "\n",
        "# Example usage:\n",
        "folder_path = 'mistral-qlora-finetune'  # Replace with your folder path\n",
        "output_path = 'mistral-qlora-finetune.zip'  # Replace with the zip file path\n",
        "zip_with_progress(folder_path, output_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5549a90d-ede9-427f-9a0c-603ef840092a",
      "metadata": {
        "id": "5549a90d-ede9-427f-9a0c-603ef840092a"
      },
      "outputs": [],
      "source": [
        "from IPython.display import FileLink\n",
        "\n",
        "# Path to the zipped file\n",
        "output_path = 'mistral-finetune.zip'\n",
        "\n",
        "# Display a download link\n",
        "FileLink(output_path)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm2",
      "language": "python",
      "name": "llm2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}